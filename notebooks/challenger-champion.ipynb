{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Results\n",
    "\n",
    "First, let's collect the performance of our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m namespace(n)\n\u001b[1;32m     10\u001b[0m run \u001b[39m=\u001b[39m Flow(\u001b[39m'\u001b[39m\u001b[39mTitanicSurvivalPredictor\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mlatest_successful_run\n\u001b[0;32m---> 11\u001b[0m acc_score \u001b[39m=\u001b[39m run\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39mscore\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m acc_score \u001b[39m>\u001b[39m best_score:\n\u001b[1;32m     13\u001b[0m     best_score \u001b[39m=\u001b[39m acc_score\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "from metaflow import Flow, namespace\n",
    "\n",
    "# these values are unique to your deployment!\n",
    "CHAMPION_MODEL_NAMESPACE = \"production:mfprj-xsfdb3gtsiboqyrd-0-vqsy\" # \"production:mfprj-xsfdb3gtsiboqyrd-0-vqsy\"\n",
    "CHALLENGER_MODEL_NAMESPACE = \"production:mfprj-cfyzlfzievjlmsf4-0-tbgz\" # \"production:mfprj-cfyzlfzievjlmsf4-0-tbgz\"\n",
    "\n",
    "best_score = -1; winner = None; winner_namespace = None\n",
    "for n in [CHAMPION_MODEL_NAMESPACE, CHALLENGER_MODEL_NAMESPACE]:\n",
    "    namespace(n)\n",
    "    run = Flow('TitanicSurvivalPredictor').latest_successful_run\n",
    "    acc_score = run.data.score\n",
    "    if acc_score > best_score:\n",
    "        best_score = acc_score\n",
    "        winner = run.data.model_type\n",
    "        winner_namespace = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The winner is the baseline model, with accuracy of 61.62%. You can find the model in the flow deployed to the production:mfprj-xsfdb3gtsiboqyrd-0-vqsy namespace.\n"
     ]
    }
   ],
   "source": [
    "print(\"The winner is the {} model, with accuracy of {}%. You can find the model in the flow deployed to the {} namespace.\".format(winner, round(100*best_score, 2), winner_namespace))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, instead of evaluating on a static validation set, you may want to send production data to each model to run an A/B test before fully replacing an incumbent model. \n",
    "\n",
    "The details of doing this at scale are beyond the scope of this course, but now you have all the infrastructural tools at your disposal - and you can always practice more advanced deployment strategies for free in your Metaflow sandbox!\n",
    "\n",
    "In the next lesson, we will point you in the direction of production A/B testing. Specifically, you will restart your model server if you turned it off, and start sending traffic to the challenger model you deployed and analyzed in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full-stack-metaflow-corise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
